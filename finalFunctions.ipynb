{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "finalFunctions.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCywaqYQdi1U",
        "outputId": "7b44a89b-616a-44e8-fe7e-ca4ae9d0290c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSvXscCti72W"
      },
      "source": [
        "import sqlite3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing \n",
        "from IPython.display import display\n",
        "from sklearn import tree\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC \n",
        "from sklearn import linear_model\n",
        "from sklearn.externals import joblib \n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier \n",
        "from sklearn.model_selection import RandomizedSearchCV"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-TDreEVmzzQ"
      },
      "source": [
        "#Original Dataset\n",
        "conn = sqlite3.connect('drive/My Drive/CaseStudy1/FPA_FOD_20170508.sqlite')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Np9ue-RtrNjQ"
      },
      "source": [
        "df = pd.read_sql_query(\"SELECT * FROM fires;\", conn)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acDMFryLsC1X",
        "outputId": "c5d3b287-422a-4cc4-b6c9-d33c69a194e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Taking random 4500 rows of the entire datset to get their fire size class prediction, We can take any number of rows from any part of the data for getting the result\n",
        "dataset = df[5000:50000:10]\n",
        "dataset_func1 = dataset.drop(['FIRE_SIZE_CLASS'], axis = 1)\n",
        "print(dataset_func1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       OBJECTID  ...                                              Shape\n",
            "5000       5001  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xd0\\xb7\\x92>\\xe9\\x0...\n",
            "5010       5011  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xa0p=\\n\\xd7\\x0b\\\\\\x...\n",
            "5020       5021  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xfc\\xff\\xff\\xff\\xff...\n",
            "5030       5031  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xa4]3\\x96\\xfc\\x02^\\...\n",
            "5040       5041  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00D\\xe1z\\x14\\xae\\xef[\\...\n",
            "...         ...  ...                                                ...\n",
            "49950     49951  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00$o\\x996\\xd0\\x19^\\xc0...\n",
            "49960     49961  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\x14\\xaeG\\xe1z,^\\xc0...\n",
            "49970     49971  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xd4\\xa3p=\\n/^\\xc0\\x...\n",
            "49980     49981  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xf4\\x15R\\x1b\\xe8L^\\...\n",
            "49990     49991  ...  b'\\x00\\x01\\xad\\x10\\x00\\x00\\xf4\\x15R\\x1b\\xe8L^\\...\n",
            "\n",
            "[4500 rows x 38 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brdojqp7iLez"
      },
      "source": [
        "def DataPrediction(data):\n",
        "  test_df = pd.DataFrame() \n",
        "  for i in range(3):\n",
        "    SampleModel = joblib.load('drive/My Drive/CaseStudy1/SampleModel_'+ str(i) + '.pkl')\n",
        "    predictedValues = SampleModel.predict(data)\n",
        "    columnName = 'predict' + str(i)\n",
        "    test_df[columnName] = predictedValues\n",
        "\n",
        "  test_finalPrediction = []\n",
        "  for j in range(len(test_df)):\n",
        "    row_list = test_df.iloc[j].values.tolist()\n",
        "    majority_count = max(set(row_list) , key=row_list.count)\n",
        "    test_finalPrediction.append(majority_count)\n",
        "\n",
        "  test_finalPrediction = np.array(test_finalPrediction)\n",
        "  return(test_finalPrediction)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giux5mqyj9NI"
      },
      "source": [
        "def function1(data):\n",
        "  '''This function will give the prediction for input data given'''\n",
        "\n",
        "  print('Deleting unnecessary features......\\n')\n",
        "  del_features = ['OBJECTID', 'FOD_ID', 'FPA_ID', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER' , 'ICS_209_NAME', 'MTBS_FIRE_NAME', 'MTBS_ID', 'COMPLEX_NAME', 'DISCOVERY_DATE', 'STAT_CAUSE_DESCR', 'CONT_DATE', 'CONT_TIME', 'FIRE_SIZE', 'OWNER_DESCR', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape' ]\n",
        "  for i, item in enumerate(del_features):\n",
        "    del data[item];\n",
        "  print('Data shape is: ', data.shape, '\\n')\n",
        "\n",
        "  print('Encoding features......\\n')\n",
        "  label_encoder = preprocessing.LabelEncoder() \n",
        "  encode_features = ['SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY']\n",
        "  for j, e_item in enumerate(encode_features):\n",
        "    data[e_item] = label_encoder.fit_transform(data[e_item]) \n",
        "    data[e_item].astype('int64')\n",
        "\n",
        "  #Manually encoding states feature\n",
        "  data['STATE'] = data['STATE'].map({'AL': 0, 'AK': 1, 'AZ': 2, 'AR': 3, 'CA': 4, 'CO': 5,'CT': 6,'DE': 7,'DC': 8,'FL': 9,'GA': 10,'HI': 11,'ID': 12,'IL': 13,'IN': 14,'IA': 15,'KS': 16,'KY': 17,'LA': 18,'ME': 19,'MD': 20,'MA': 21,'MI': 22,'MN': 23,'MS': 24,'MO': 25,'MT': 26,'NE': 27,'NV': 28,'NH': 29,'NJ': 30,'NM': 31,'NY': 32,'NC': 33,'ND': 34,'OH': 35,'OK': 36,'OR': 37,'PA': 38,'PR': 39,'RI': 40,'SC': 41,'SD': 42,'TN': 43,'TX': 44,'UT': 45,'VT': 46,'VA': 47,'WA': 48,'WV': 49,'WI': 50,'WY': 51}) \n",
        "  data['STATE'].astype('int64')\n",
        "\n",
        "  print('Performing Feature Engineering......\\n')\n",
        "  #Adding Feature Discovery Month\n",
        "  discovery_month = [];\n",
        "  for i in range(len(data)):\n",
        "   key = data.iloc[i]['DISCOVERY_DOY']\n",
        "   if( 1 <= key <= 31 ):\n",
        "    discovery_month.append(1)\n",
        "   elif ( 32 <= key <= 60 ):\n",
        "      discovery_month.append(2)\n",
        "   elif ( 61 <= key <= 91 ):\n",
        "     discovery_month.append(3)\n",
        "   elif ( 92 <= key <= 121 ):\n",
        "     discovery_month.append(4)\n",
        "   elif ( 122 <= key <= 152 ):\n",
        "     discovery_month.append(5)\n",
        "   elif ( 153 <= key <= 182 ):\n",
        "     discovery_month.append(6)\n",
        "   elif ( 183 <= key <= 213 ):\n",
        "     discovery_month.append(7)\n",
        "   elif ( 214 <= key <= 244 ):\n",
        "     discovery_month.append(8)\n",
        "   elif ( 245 <= key <= 274 ):\n",
        "     discovery_month.append(9)\n",
        "   elif ( 275 <= key <= 305 ):\n",
        "     discovery_month.append(10)\n",
        "   elif ( 306 <= key <= 335 ):\n",
        "     discovery_month.append(11)\n",
        "   elif ( 336 <= key <= 366 ):\n",
        "     discovery_month.append(12)\n",
        "    \n",
        "  data['DISCOVERY_MONTH'] = discovery_month\n",
        "  data['DISCOVERY_MONTH'].astype('int64')\n",
        "  print('Data shape is: ', data.shape, '\\n')\n",
        "\n",
        "  #Delete DISCOVERY_DOY and CONT_DOY also now\n",
        "  del data['DISCOVERY_DOY']\n",
        "  del data['CONT_DOY']\n",
        "\n",
        "  #Feature2 DISCOVERY_TOD\n",
        "  discovery_tod = [];\n",
        "  data['DISCOVERY_TIME'] = data['DISCOVERY_TIME'].replace([None],'0000')\n",
        "  for i in range(len(data)):\n",
        "    key = data.iloc[i]['DISCOVERY_TIME']\n",
        "    if( key == '0000' ):\n",
        "      discovery_tod.append(0)\n",
        "    elif ( '0000' < key <= '0600' ):\n",
        "      discovery_tod.append(1)\n",
        "    elif ( '0600' < key <= '1200' ):\n",
        "      discovery_tod.append(2)\n",
        "    elif ( '1200' < key <= '1600' ):\n",
        "      discovery_tod.append(3)\n",
        "    elif ( '1600' < key <= '2000' ):\n",
        "      discovery_tod.append(4)\n",
        "    elif ( '2000' < key <= '2400' ):\n",
        "      discovery_tod.append(5)\n",
        "\n",
        "  data['DISCOVERY_TOD'] = discovery_tod\n",
        "  data['DISCOVERY_TOD'].astype('int64')\n",
        "\n",
        "  del data['DISCOVERY_TIME']\n",
        "\n",
        "  data['LATITUDE'] = (data['LATITUDE']*10).apply(np.floor)/10\n",
        "  data['LONGITUDE'] = (data['LONGITUDE']*10).apply(np.floor)/10\n",
        "\n",
        "  #Add forest Area feature\n",
        "  forest_Area = pd.read_excel('drive/My Drive/CaseStudy1/FOREST_Area.xlsx')\n",
        "  forest_Area.head()\n",
        "  STATE_PRCNT_FOREST = [];\n",
        "  for i in range(len(data)):\n",
        "    key = data.iloc[i]['STATE'].astype('int64')\n",
        "    STATE_PRCNT_FOREST.append(forest_Area['Forest_Coverage'].values[key])\n",
        "  \n",
        "  data['STATE_PRCNT_FOREST'] = STATE_PRCNT_FOREST\n",
        "  data['STATE_PRCNT_FOREST'].astype('float64')\n",
        "\n",
        "  #Add Avg Temp Feature\n",
        "  avg_temp  = pd.read_excel('drive/My Drive/CaseStudy1/avg_temp.xlsx')\n",
        "\n",
        "  AVG_TEMP_LIST = [];\n",
        "  for i in range(len(data)):\n",
        "    state_key = data.iloc[i]['STATE'].astype('int64')\n",
        "    year_key = data.iloc[i]['FIRE_YEAR'].astype('int64')\n",
        "    AVG_TEMP_LIST.append(avg_temp[year_key].values[state_key])\n",
        "  \n",
        "  data['AVG_TEMP'] = AVG_TEMP_LIST\n",
        "  data['AVG_TEMP'].astype('float64')\n",
        "\n",
        "  #Add Avg Prec Feature\n",
        "  avg_prec  = pd.read_excel('drive/My Drive/CaseStudy1/avg_prec.xlsx')\n",
        "\n",
        "  AVG_PREC_LIST = [];\n",
        "  for i in range(len(data)):\n",
        "   state_key = data.iloc[i]['STATE'].astype('int64')\n",
        "   AVG_PREC_LIST.append(avg_prec['Avg_Prec'].values[state_key])\n",
        "\n",
        "  data['AVG_PREC'] = AVG_PREC_LIST\n",
        "  data['AVG_PREC'].astype('float64')\n",
        "  print('Final features are: ', data.columns,'\\n')\n",
        "  print('EDA Completed...... \\n')\n",
        "  print('Predicting the fire size class......\\n')\n",
        "\n",
        "  predictions = DataPrediction(data)\n",
        "  data['PREDICTED_CLASS'] = predictions\n",
        "  #Simplifying the predicted class by giving area covered in each class\n",
        "  predictedRange = []\n",
        "  for i in range(len(data)):\n",
        "    key = data.iloc[i]['PREDICTED_CLASS']\n",
        "    if( key == 1 ):\n",
        "      predictedRange.append('0-0.25 acres')\n",
        "    elif ( key == 2 ):\n",
        "      predictedRange.append('0.26-9.9 acres')\n",
        "    elif ( key == 3 ):\n",
        "      predictedRange.append('10.0-99.9 acres')\n",
        "    elif ( key == 4 ):\n",
        "      predictedRange.append('100-299 acres')\n",
        "    elif ( key == 5 ):\n",
        "      predictedRange.append('300-999 acres')\n",
        "    elif ( key == 6 ):\n",
        "      predictedRange.append('1000-5000 acres')\n",
        "    else:\n",
        "      predictedRange.append('5000+ acres')\n",
        "    \n",
        "  data['Area Range'] = predictedRange\n",
        "\n",
        "  print(data)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W62ObGNOkk3H",
        "outputId": "9a3ecc9f-46d4-4a26-8798-8beff8b3bc29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "function1(dataset_func1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Deleting unnecessary features......\n",
            "\n",
            "Data shape is:  (4500, 12) \n",
            "\n",
            "Encoding features......\n",
            "\n",
            "Performing Feature Engineering......\n",
            "\n",
            "Data shape is:  (4500, 13) \n",
            "\n",
            "Final features are:  Index(['SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY',\n",
            "       'FIRE_YEAR', 'STAT_CAUSE_CODE', 'LATITUDE', 'LONGITUDE', 'OWNER_CODE',\n",
            "       'STATE', 'DISCOVERY_MONTH', 'DISCOVERY_TOD', 'STATE_PRCNT_FOREST',\n",
            "       'AVG_TEMP', 'AVG_PREC'],\n",
            "      dtype='object') \n",
            "\n",
            "EDA Completed...... \n",
            "\n",
            "Predicting the fire size class......\n",
            "\n",
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 110 out of 110 | elapsed:    0.1s finished\n",
            "       SOURCE_SYSTEM_TYPE  SOURCE_SYSTEM  ...  PREDICTED_CLASS      Area Range\n",
            "5000                    0              0  ...                1    0-0.25 acres\n",
            "5010                    0              0  ...                1    0-0.25 acres\n",
            "5020                    0              0  ...                2  0.26-9.9 acres\n",
            "5030                    0              0  ...                1    0-0.25 acres\n",
            "5040                    0              0  ...                1    0-0.25 acres\n",
            "...                   ...            ...  ...              ...             ...\n",
            "49950                   0              0  ...                1    0-0.25 acres\n",
            "49960                   0              0  ...                1    0-0.25 acres\n",
            "49970                   0              0  ...                1    0-0.25 acres\n",
            "49980                   0              0  ...                1    0-0.25 acres\n",
            "49990                   0              0  ...                1    0-0.25 acres\n",
            "\n",
            "[4500 rows x 16 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vN0JCQlxIRY"
      },
      "source": [
        "Result: 2 columns namely 'PREDICTED_CLASS' and 'Area Range' are the results obtained for the given set of features in our model. \n",
        "We can change the number of input features as per our convinience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9isRw4kpxafH"
      },
      "source": [
        "#Function 2: Taking both x and y values to get the the Performance Metric values for our given data\n",
        "def function2(dataset):\n",
        "  '''Taking the entire dataset as input for this function and then using the labels to determine MAE and MAPE scores'''\n",
        "  #Encoding y_data for computation purpose\n",
        "  dataset['FIRE_SIZE_CLASS'] = dataset['FIRE_SIZE_CLASS'].map({'A': 1, 'B': 2, 'C':3, 'D':4, 'E': 5, 'F': 6,'G': 7}) \n",
        "  dataset['FIRE_SIZE_CLASS'].astype('int64')\n",
        "\n",
        "  #Breaking down  features and label data \n",
        "  \n",
        "  y_data = dataset['FIRE_SIZE_CLASS']\n",
        "  x_data = dataset.drop(['FIRE_SIZE_CLASS'], axis = 1)\n",
        "  \n",
        "  #Predicing labels using x_data (Similar to function 1)\n",
        "  del_features = ['OBJECTID', 'FOD_ID', 'FPA_ID', 'NWCG_REPORTING_UNIT_ID', 'NWCG_REPORTING_UNIT_NAME', 'SOURCE_REPORTING_UNIT', 'SOURCE_REPORTING_UNIT_NAME', 'LOCAL_FIRE_REPORT_ID', 'LOCAL_INCIDENT_ID', 'FIRE_CODE', 'FIRE_NAME', 'ICS_209_INCIDENT_NUMBER' , 'ICS_209_NAME', 'MTBS_FIRE_NAME', 'MTBS_ID', 'COMPLEX_NAME', 'DISCOVERY_DATE', 'STAT_CAUSE_DESCR', 'CONT_DATE', 'CONT_TIME', 'FIRE_SIZE', 'OWNER_DESCR', 'COUNTY', 'FIPS_CODE', 'FIPS_NAME', 'Shape' ]\n",
        "  for i, item in enumerate(del_features):\n",
        "    del x_data[item];\n",
        "\n",
        "\n",
        "  label_encoder = preprocessing.LabelEncoder() \n",
        "  encode_features = ['SOURCE_SYSTEM_TYPE', 'SOURCE_SYSTEM', 'NWCG_REPORTING_AGENCY']\n",
        "  for j, e_item in enumerate(encode_features):\n",
        "    x_data[e_item] = label_encoder.fit_transform(x_data[e_item]) \n",
        "    x_data[e_item].astype('int64')\n",
        "\n",
        "  #Manually encoding states feature\n",
        "  x_data['STATE'] = x_data['STATE'].map({'AL': 0, 'AK': 1, 'AZ': 2, 'AR': 3, 'CA': 4, 'CO': 5,'CT': 6,'DE': 7,'DC': 8,'FL': 9,'GA': 10,'HI': 11,'ID': 12,'IL': 13,'IN': 14,'IA': 15,'KS': 16,'KY': 17,'LA': 18,'ME': 19,'MD': 20,'MA': 21,'MI': 22,'MN': 23,'MS': 24,'MO': 25,'MT': 26,'NE': 27,'NV': 28,'NH': 29,'NJ': 30,'NM': 31,'NY': 32,'NC': 33,'ND': 34,'OH': 35,'OK': 36,'OR': 37,'PA': 38,'PR': 39,'RI': 40,'SC': 41,'SD': 42,'TN': 43,'TX': 44,'UT': 45,'VT': 46,'VA': 47,'WA': 48,'WV': 49,'WI': 50,'WY': 51}) \n",
        "  x_data['STATE'].astype('int64')\n",
        "\n",
        "  #Adding Feature Discovery Month\n",
        "  discovery_month = [];\n",
        "  for i in range(len(x_data)):\n",
        "   key = x_data.iloc[i]['DISCOVERY_DOY']\n",
        "   if( 1 <= key <= 31 ):\n",
        "    discovery_month.append(1)\n",
        "   elif ( 32 <= key <= 60 ):\n",
        "      discovery_month.append(2)\n",
        "   elif ( 61 <= key <= 91 ):\n",
        "     discovery_month.append(3)\n",
        "   elif ( 92 <= key <= 121 ):\n",
        "     discovery_month.append(4)\n",
        "   elif ( 122 <= key <= 152 ):\n",
        "     discovery_month.append(5)\n",
        "   elif ( 153 <= key <= 182 ):\n",
        "     discovery_month.append(6)\n",
        "   elif ( 183 <= key <= 213 ):\n",
        "     discovery_month.append(7)\n",
        "   elif ( 214 <= key <= 244 ):\n",
        "     discovery_month.append(8)\n",
        "   elif ( 245 <= key <= 274 ):\n",
        "     discovery_month.append(9)\n",
        "   elif ( 275 <= key <= 305 ):\n",
        "     discovery_month.append(10)\n",
        "   elif ( 306 <= key <= 335 ):\n",
        "     discovery_month.append(11)\n",
        "   elif ( 336 <= key <= 366 ):\n",
        "     discovery_month.append(12)\n",
        "    \n",
        "  x_data['DISCOVERY_MONTH'] = discovery_month\n",
        "  x_data['DISCOVERY_MONTH'].astype('int64')\n",
        "\n",
        "  #Delete DISCOVERY_DOY and CONT_DOY also now\n",
        "  del x_data['DISCOVERY_DOY']\n",
        "  del x_data['CONT_DOY']\n",
        "\n",
        "  #Feature2 DISCOVERY_TOD\n",
        "  discovery_tod = [];\n",
        "  x_data['DISCOVERY_TIME'] = x_data['DISCOVERY_TIME'].replace([None],'0000')\n",
        "  for i in range(len(x_data)):\n",
        "    key = x_data.iloc[i]['DISCOVERY_TIME']\n",
        "    if( key == '0000' ):\n",
        "      discovery_tod.append(0)\n",
        "    elif ( '0000' < key <= '0600' ):\n",
        "      discovery_tod.append(1)\n",
        "    elif ( '0600' < key <= '1200' ):\n",
        "      discovery_tod.append(2)\n",
        "    elif ( '1200' < key <= '1600' ):\n",
        "      discovery_tod.append(3)\n",
        "    elif ( '1600' < key <= '2000' ):\n",
        "      discovery_tod.append(4)\n",
        "    elif ( '2000' < key <= '2400' ):\n",
        "      discovery_tod.append(5)\n",
        "\n",
        "  x_data['DISCOVERY_TOD'] = discovery_tod\n",
        "  x_data['DISCOVERY_TOD'].astype('int64')\n",
        "\n",
        "  del x_data['DISCOVERY_TIME']\n",
        "\n",
        "  x_data['LATITUDE'] = (x_data['LATITUDE']*10).apply(np.floor)/10\n",
        "  x_data['LONGITUDE'] = (x_data['LONGITUDE']*10).apply(np.floor)/10\n",
        "\n",
        "  #Add forest Area feature\n",
        "  forest_Area = pd.read_excel('drive/My Drive/CaseStudy1/FOREST_Area.xlsx')\n",
        "  forest_Area.head()\n",
        "  STATE_PRCNT_FOREST = [];\n",
        "  for i in range(len(x_data)):\n",
        "    key = x_data.iloc[i]['STATE'].astype('int64')\n",
        "    STATE_PRCNT_FOREST.append(forest_Area['Forest_Coverage'].values[key])\n",
        "  \n",
        "  x_data['STATE_PRCNT_FOREST'] = STATE_PRCNT_FOREST\n",
        "  x_data['STATE_PRCNT_FOREST'].astype('float64')\n",
        "\n",
        "  #Add Avg Temp Feature\n",
        "  avg_temp  = pd.read_excel('drive/My Drive/CaseStudy1/avg_temp.xlsx')\n",
        "\n",
        "  AVG_TEMP_LIST = [];\n",
        "  for i in range(len(x_data)):\n",
        "    state_key = x_data.iloc[i]['STATE'].astype('int64')\n",
        "    year_key = x_data.iloc[i]['FIRE_YEAR'].astype('int64')\n",
        "    AVG_TEMP_LIST.append(avg_temp[year_key].values[state_key])\n",
        "  \n",
        "  x_data['AVG_TEMP'] = AVG_TEMP_LIST\n",
        "  x_data['AVG_TEMP'].astype('float64')\n",
        "\n",
        "  #Add Avg Prec Feature\n",
        "  avg_prec  = pd.read_excel('drive/My Drive/CaseStudy1/avg_prec.xlsx')\n",
        "\n",
        "  AVG_PREC_LIST = [];\n",
        "  for i in range(len(x_data)):\n",
        "   state_key = x_data.iloc[i]['STATE'].astype('int64')\n",
        "   AVG_PREC_LIST.append(avg_prec['Avg_Prec'].values[state_key])\n",
        "\n",
        "  x_data['AVG_PREC'] = AVG_PREC_LIST\n",
        "  x_data['AVG_PREC'].astype('float64')\n",
        "\n",
        "  predictions = DataPrediction(x_data)\n",
        "\n",
        "  #Got the prediction values, now computing the errors\n",
        "  MAE_value = mean_absolute_error(y_data, predictions)\n",
        "  print('Mean Absolute Error comes out to be: ', MAE_value, '\\n')\n",
        "\n",
        "  y_true, y_pred = np.array(y_data), np.array(predictions)\n",
        "  MAPE_value = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "  print('Mean Absolute Percentage Error is: ', MAPE_value)\n",
        "\n",
        "\n",
        "  \n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SqqJNao5DhH",
        "outputId": "221f8667-e7b4-4dd3-aee1-218cfd3a48e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "function2(dataset)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   7 out of   7 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  11 out of  11 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  13 out of  13 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  14 out of  14 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  17 out of  17 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  19 out of  19 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  21 out of  21 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  22 out of  22 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  23 out of  23 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  24 out of  24 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  25 out of  25 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  27 out of  27 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  29 out of  29 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  31 out of  31 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  32 out of  32 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  33 out of  33 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  34 out of  34 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  35 out of  35 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  36 out of  36 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  37 out of  37 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  38 out of  38 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  39 out of  39 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  41 out of  41 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  42 out of  42 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  43 out of  43 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  44 out of  44 | elapsed:    0.0s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  45 out of  45 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  46 out of  46 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  47 out of  47 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  48 out of  48 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done  49 out of  49 | elapsed:    0.1s remaining:    0.0s\n",
            "[Parallel(n_jobs=1)]: Done 110 out of 110 | elapsed:    0.1s finished\n",
            "Mean Absolute Error comes out to be:  0.5002222222222222 \n",
            "\n",
            "Mean Absolute Percentage Error is:  23.428306878306877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB_SuHVT-Wls"
      },
      "source": [
        "Mean Absolute Percentage Error (MAPE) is the most common error for Forcasting.\n",
        "In our study the MAPE value comes out to be 23.42%\n",
        "which means for the remaining ~ 77% of the times, the model is predicting the right firesize class.\n"
      ]
    }
  ]
}